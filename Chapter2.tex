% !TEX root = Thesis.tex

\chapter{Basics}
\label{chap:basics}
\minitoc

\section{Introduction}

%\begin{quotation}
%Most of us are brought up to feel that what we see out in front of us is something that lies beyond our eyes, out there. That the colors and the shapes that you see in this room are out there. In fact, that is not so. In fact, all that you see is a state of affairs inside your head. All these colors, all these lights, are conditions of the optical nervous system. There are, outside the eyes, quanta, electronic phenomena, vibrations, but these are not light, they are not colors until they are translated into states of the human nervous system. So if you want to know how the inside of your head feels, open your eyes and look. That is how the inside of your head feels\\
%\textsc{Alan Watts}
%\end{quotation}

% What is the unit of perception that I can use across modalities and use in collage practice
% Global vs. Local in audio/vision
% Could do xyz, but did it this way...

%take examples in chap 1, what are unit, how does it approx percept of whole; how artists have thought about it... 

% pip/pop/ deviant

% road map to binding /crossmodal

% object files (wolfe), geons (biederman), indexical objects (pylyshyn), gestalts... need to motivate a process for discovering representations that are likely to be the units of attention

%Beginning with seminal theories of visual perception such as ``Gestalts'', ``proto-objects'' and ``Geons'' and seminal studies in Gist and Change Blindness, reinforcing what is known about early representations in visual perception, this chapter will then proceed into literature discussing auditory perception, focusing on research in Auditory Scene Analysis, fMRI, and the recording of electrical activity of the brain and Event Related Potentials (ERPs), leading to a discussion of representation in audition.  Finally, the review concludes in a discussion on developing a computational model of early perception using the presented theories.  

Our perception of the world is comprised of objects, events, and meaningful entities.  Yet, the sensory information we use to constitute these percepts are built from physical signals that have no meaning by themselves.  The challenge our perceptual machinery must face is associating this noisy incoming stimuli to meaningful entities we have already learned so that we may identify and utilize them.  This challenge was perhaps first described in the work of 19th-century empiricist, Hermann von Helmholtz, who suggested that the mind evaluates sensations through ``unconscious inferences'' which combine the stimulus of the world with prior notions of it to form our final perception. 

Numerous theories since then have proposed a representational framework for perception.  These are generally described by representations that are inaccessible to our conscious awareness similar to Helmholtz's inferences.  As we do not have direct access to them, what might an unconscious representations supporting perception look or sound like?  What can they explain, and what do they lack the ability to explain?  It is the aim of this thesis to build a better understanding of questions such as these through an arts practice focused on computational approaches to sonic and visual collage processes.  Within this collage-based practice, the units being assembled will be modeled based on the theoretical foundations laid in this chapter.  

This chapter is thus dedicated to reviewing what is known about unconscious representations in auditory and visual perception.  As this field is incredibly vast with many non-overlapping research areas, this review will sacrifice brevity for more depth in fewer areas of research.  Beginning with seminal work investigating auditory perception before moving onto visual perception, this chapter will primarily describe research making use of electrophysiological and eye-tracking techniques in the domains of cognitive neuroscience and behavioral psychology.  These reviews will set the groundwork for a theory of unconscious memory representations in auditory and visual perception which will be computationally modeled and used in practice in the later chapters of this thesis.   

%As we do not have direct access to them, what do these representations look or sound like?  What can they explain, and what do they lack the ability to explain?  It is the aim of this thesis to build an understanding of perception through an arts practice focused on computational approaches to sonic and visual collage processes.  Within this collage-based practice, the units being assembled are modeled based on a theory of representations supporting perception motivated in this chapter.  The final aim of this practice is to open a dialogue around questions of representation within perception, thus reflecting back onto the theories set forth in this chapter.  


\section{Audition}

We are capable of selectively listening to one of multiple sounds in an environment cluttered with simultaneous events, a feat known more colloquially as the cocktail-party problem \cite{McDermott2009}.  Though we do not have eyes to move to different parts of an auditory scene, the cochlea of the inner ear can effectively break down a complex time-varying set of frequencies from a natural environment into a set of narrow, ``critical bands'' \cite{Fletcher1940}.  From the energy in these bands, understanding how we represent and understand such scenes within the auditory modality has been the main challenge of neuroscientists and psychologists investigating auditory perception.  

\subsection{Auditory Object}

Certainly one challenge has been to define what the entities that compose the auditory scene may be described by within the auditory modality.  As Winkler points out \cite{Ist2010}, the notion of an object is highly guided by our visual experiences, and even the Merriam-Webster Dictionary defines object as, ``something that may be seen or felt'' (as quoted in \cite{Ist2010}) or as the Oxford English Dictionary puts it, ``something placed before or presented to the eyes or other senses'' (as quoted in \cite{Griffiths2004}).  Perhaps a more useful definition of an object or of object-ness entails the capability of the brain to represent, attend, and understand complex and dynamic stimuli across what Marr in visual terms understood as variances in a representation \cite{Marr1982}.  In other words, the perceivable object must be represented by some separable aspect of the environment.  

Griffiths notes that objects perceived must originate in the world, or else they are labeled as hallucinations or ``errors'' in processing in the brain.  Considering this fact alone might lead one to consider the auditory object as information leading to the \textit{source} of the object in the external world.  However, as Griffith continues to elaborate, the auditory object may also characterize information of an \textit{event} in the external world, and not necessarily provide information leading to its source or discrimination from other aspects in the current environment.  As an example, consider a voice in a crowd.  The source may be recognized as a particular speaker, lending information to the source of the sound and even where the sound occurred in space by matching to one's visual knowledge.  However, instead of the speaker, the vowels of the speaker may also be perceived, thus representing not the source of the speaker, but the auditory patterns of changes produced by different possible sources and in different possible environments \cite{Griffiths2004}.  

It is perhaps due to the complex nature of defining objects in audition that the majority of research in understanding auditory perception has focused on simple un-naturalistic acoustic scenes.  These scenes are generally composed of sine tones where simple physical parameters such as the tone's pitch or its temporal frequency may be altered.  Across modalities, this method of research has been denoted as ``psychophysical'', as the aim is to investigate the relationship of varying properties of a stimulus along one or more physical dimensions to a subject's experience or behavior \cite{}.  

\subsection{Auditory Scene Analysis}

A seminal starting point into the study of auditory representation comes from psychophysical research in ``streaming''.  Van Noorden in 1975 demonstrated the bi-stability of perception when listening to alternating tones composed of sine waves \cite{Noorden1975}.  Depending on the tone's temporal frequency of onset, $\Delta T$, and difference in pitch, $\Delta F$, participants would perceive one of three possible scenarios: two separate streams (A tones and B tones, the \textit{segregated} percept); one stream (A and B tones, the \textit{integrated} percept); or a mixture of streams (the \textit{ambiguous} percept).  The integrated case is mostly demonstrated for a low $\Delta F$, the segregated case for high $\Delta F$ and/or $\Delta T$, and the ambiguous case for values in between.  Van Noorden describes the border between ambiguous and segregated percepts as the \textit{temporal coherence} boundary.  

Defining this boundary in greater detail has been the focus of research in the field of Auditory Scene Analysis for the last 30 years, a term coined by Albert Bregman \cite{Bregman1990}.  Approaches in Auditory Scene Analysis argue that the perceptual organization of an auditory scene is represented by a decomposition into pre-attentive auditory streams.  These streams are thought to be pre-attentive as we do not have conscious awareness of them.  For instance, if we are able to segregate sounds within a complex natural environment into one where we can describe a person's voice and what they are saying, then we will not be able to report information about other sounds that occurred at the same time in the environment.  In other words, the segregated stream that we can report has been selected or \textit{foregrounded} by attention, bringing it to conscious awareness, while the stream we are unaware of has been \textit{backgrounded}.  

Bregman argues that before streams are foregrounded, they are encoded by one of two formations: (1) rapidly available cues from primitive, low-level characteristics such as frequency, intensity, and location; and (2), schema-driven integration of sensory evidence where schemas are defined by Gestalt-like regularities such as similarities, differences, common-fate, or continuity in frequency information from a continuous signal.  In the literature, these formations have also been denoted as \textit{simultaneous} and \textit{sequential} grouping strategies \cite{Winkler2009a}.  Simultaneous cues suggest that sounds are likely to be segregated if their physical characteristics despite their context are significantly varied.  As a result, these cues are independent of the attention of a listener and are purely based on the cues in the environment.  In contrast, sequential cues require a listener to have formed the notion of a representation, as the notion of regularity is based on previous listening.  These cues are more favorable in environments where very noisy conditions require precise knowledge of the source to attend to, thus resolving any ambiguities by using existing representations to segregate sources (e.g. the classic ``cocktail party problem'').  

Bregman's theory provides a basis for understanding some features of auditory objects as they may appear within unconscious memory representations.  Rather than the notion of object, Bregman's notion of streams provide a perceptual representation as determined during auditory scene analysis.  This analysis is described in the form of a stimulus-driven cue and a cognitively-driven Gestalt-like cue.  Research supporting the encoding of both formations has seen great support through investigations making use of \textit{electroencephalograph} (EEG) recordings.  

\subsection{Electrophysiology}

Before discussing research in electrophysiology, it is important to have a basic background in some of the techniques and seminal research with EEG.  EEG recordings, first described by Hans Berger in 1929, attempt to non-invasively infer brain activity by measuring a time varying signal of the electrical activity on the scalp of the head.  One of the benefits of EEG comes with its high temporal resolution often on the order of milliseconds.  To localize or disassociate changes in electrical activity in specific brain regions, more electrodes may also be used (upwards to 128).  As the raw electrical signal coming to electrodes are very low amplitude, the signal is first amplified.  However, the incredibly complex nature of the brain means that looking at the amplified signal may not be very useful by itself.  A presiding technique of measuring brain activity using EEG in cognitive studies is the \textit{event-related potential} (ERP).  ERPs are simply the measured neural activity time-locked to the presentation of a stimulus.  By having many presentations, or trials, of the same stimulus, the average wave can be computed across trials and studied in greater detail.  ERPs are generally described in terms of specific components that contribute to the averaged signal.  The nomenclature of some of the more basic components are based on their polarity (positive deflection, P; negative deflection, N) and how long after a stimulus they occur (e.g. N1 or N100 means a negative deflection around 100 ms after stimulus presentation).  
%s a result, many methods have been developed to express more meaningful measurements, such as measuring event related synchronization (ERS), which measures the magnitudes of different frequencies over time or component analysis, which attempts to re-project the EEG data into a set of hidden factors.  However, the 
%Some components are further named based on hemispheric considerations, such as a stimuli presented to the left side of the participant producing an ERP in the right hemisphere of the brain (contralateral, C), and where in the brain the ERP is generally found (e.g. a posterior or anterior contralateral component for vision, N2PC, or for audio, N2AC, respectively \cite{Gamble2011}).

One of the first well studied components within auditory processing is the N1 component which has been shown to be elicited during the onset of a stimulus (usually from silence to sound).  Perhaps first described in 1939, Davis described a sound-evoked change in the EEG recordings of the waking human brain \cite{Davis1939}.  The onset of a tone evoked a negative wave 100-150 ms after onset near the vertex of the scalp.  This was again described in a study in 1965 in which a flash of light or sound would proceed another flash of light or sound \cite{Sutton1965}.  Sutton et al. describes a negative potential peaking at 110 ms (N1) at the vertex of the scalp and a late positive potential at about 300 ms for sound and 340 ms for light (P3).  

Hillyard et al.'s 1973 study expanded on these previous results, showing that the N1 component is of even greater magnitude when a subject's selective attention is required in detecting a target tone \cite{Hillyard1973}.   In their study, a subject was presented random tones in either ear with very short irregular inter-stimulus intervals (ISIs, referring to the $\Delta T$), with higher pitch tones in the left than the right ear, and with randomly placed target tones with a slightly higher pitch in both ears.  The subject's task was to count the randomly placed tones in a target ear, meaning attention should have diverted to the target ear ignoring the other ear.  As a result, Hillyard et al. was able to demonstrate that the N1 component measured at the vertex was higher of higher amplitude (around 60-70 ms in most subjects) for events in the attended ear.  They suggested their results reflected an enhancement of the N1 component.  They also demonstrated an evoked late positive component peaking at 250 to 400 ms (P3) in the attended ear only when the target tone was presented, indicating the selective recognition of the target tone from which any subsequent cognitive or motor activities could follow (e.g. counting of the tones). 

However, their result was later reinterpreted in another study which used longer ISIs of 800 ms \cite{Naatanen1978}.  Their study demonstrated that a new component, which they denoted the \textit{processing negativity} (PN), emerged exhibiting the effect of selective attention, rather than any enhancement of the N1.  This effect was found as a slow negative shift appearing at 150 ms and lasting 500 ms, rather than an earlier enhancement of the N1.  Their results indicate that the increased amplitude of the N1 in the attended ear could have been due to the short ISIs causing an overlap between the PN and N1 components, though even this explanation is still open to interpretation as a genuine enhancement of the N1 component can occur for strongly focused selective attention \cite{Naatanen1978,Hillyard1983,Naatanen2011}.  In any case, it seems the N1 is well described by the literature as an onset detector for auditory stimuli thus providing the awareness of a stimulus.  The PN may then provide an attentional-trace function, and be evoked during a template match during an attentional task requiring stimulus selection.  

In 1976, another now well-known combination of components, the N2-P3a \cite{Snyder1976}, was demonstrated.  Unlike the N1 component which is based on the simple onset of a stimulus, the N2 was generated whenever a deviant or infrequent stimuli was presented.  This difference becomes even greater with larger differences in Hz between the standard and deviant (e.g. 80\% chance of a 1000 Hz tone versus deviants of 1020 or 1040 Hz at 20\%).  This entailed the notion that some representation had been created in order to compare it to the deviant stimuli.  Unlike the PN, the deviant did not need to be known or recognized based on prior training, or require selective attention, though its characteristics could effectively demand attention, hence evoking a P3a component peaking around 258 ms.  In contrast, when the subject was actively attending, a later P3 component denoted the P3b was evoked, peaking around 378 ms, still with the N2 component preceding it. 

The N2 component has since been very well studied, as it entails that we are able to represent sounds and compare them to the ongoing environment even outside of a given task.  In particular, a re-interpretation of the 1976 study in 1978 showed that when subtracting the deviant trials from the standard ones, the remaining N2 spike (``N2a'') demonstrates the brains capability to detect irregular changes in what the authors originally called ``template mismatch'' \cite{Naatanen1978}.  Importantly, these changes are not dependent on the deviant stimuli's features itself, but only in relation to the standard, thus providing evidence for the brain's capability for segregation via a sequential or schema-based grouping cue.  This subtraction was later denoted as the \textit{mismatch negativity} (MMN), and has been shown to appear anywhere from 100 - 250 ms after onset of a deviating stimuli \cite{Naatanen1978,Naatanen1987,Naatanen2007,Campbell2007,Garrido2009,Naatanen2011}.  Such relational negativities were also capable of appearing within the time-frame of the N1, leading it to be classified as a part of the N1 \cite{Naatanen1987}.  Though only recently it has been discriminated from the N1 \cite{Campbell2007}.  

Expanding on the MMN further, more than low-level attributes such as pitch, temporal frequency, intensity, or duration, this same pattern of behavior is exhibited for a wide-range of stimuli even including higher-order or more complex violations such as grammar in mother-tongue sentences \cite{} or temporal sequences such as violations in patterns of ascending/descending tones \cite{}.   Though perhaps the most striking feature of the MMN is that it demonstrates that attention is not required for the brain to detect irregular patterns in the environment.  For instance, in 1978 study mentioned above, subjects were instructed to read a book during the passive listening trials and were unaware of any difference in tones, yet a MMN was still evoked \cite{} - (have to check if this is true, but I think it is).

When investigating measurements of MMN during segmentation and integration of sounds, the same pattern of activations are shown to exist for a single source as are for multiple sources, even when the sources share similar frequencies, suggesting that segmentation occurs before integration \cite{Sussman2005}. Such research suggests that pre-attentive processes within 150 ms provide a coarse shape or texture necessary for recognition, an auditory gist, and only the details that are necessary to the task at hand are further integrated \cite{Harding2007}.  Their research gives further support for the assumed temporal window of integration of 170 ms as shown by...\cite{}.

N1-P2; Negative potential, ORN, at 180 ms, in both active and passive conditions, followed by positive peak at 450 ms during active listening.  Alain... \cite{Alain2001}

\subsection{Change Deafness}

Research in representations forming outside of selective attention has been given additional support through the phenomena of change deafness, termed similarly to change blindness as it shares some similarities (discussed in Section \ref{sec:change-blindness}) \cite{Vitevitch2000}.  Participants were instructed to repeat out loud a list of words as heard coming from one ear of their headphones, and to ignore the words presented in the other.  The words range from lexically easy words (high word frequency and few words are similar sounding) to lexically hard words (low word frequency, many words sound similar to it, and the similar sounding words have a high frequency of occurrence).  Halfway through the list of words, participants were given a 1 minute break, and the speaker's voice would change on only one of the trials.  42\% of participants (5/12) were unable to detect the change in speaker.  Unlike participants that did not detect the change, participants detecting the change had faster reaction times in repeating lexical words with decreased lexical difficulty though were unaware of the change in the speaker's voice.  This suggests that in order to detect the change, some increase in processing and hence the increased reaction time in repeating the word was required.  This increase in processing suggested that attention towards the auditory stimuli was required for detecting the change.

Research demonstrating change deafness opens questions into the nature of templates effecting MMN and PN.  The results could be interpreted that the original speaker had provided a template match providing a basis for segregation and hence the decreased reaction time, but had not been integrated to conscious awareness and hence no awareness of the different voice.  Another interpretation could be that the template is integrated, but cannot be automatically compared.  More recent research has expanded on its result showing that transients of white-noise or silence were not necessary to produce change deafness, unlike its visual counterpart \cite{TURATTO2008}.  Their results suggest the short-term auditory memory capacities were the main deciding factor of change deafness.  

\subsection{Brain Imaging}

How this process occurs neurologically in audition has only recently been investigated. In a study employing fMRI of subjects listening to random note sequences, activations shown in IPS and STS occur during ``figures'' of a less random nature than the preceding and following note sequences, though participants are unable to report hearing a figure, suggesting pre-attentive binding of spectral-time course information occurs during listening \cite{Teki2011a}. As fMRI scans typically take on the order of 2 seconds per scan, understanding if a subject is aware of a figure can only be done qualitatively after listening.  However, recent research investigating the attentional mechanisms involved in time-course encoding of auditory streaming paradigms often also make use of EEG recordings as the temporal precision required in understanding if and when attention acts upon a figure is on the order of milliseconds.  For example, evidence for the predictive encoding in time-course frequency information \cite{Winkler2009} give further evidence for the predictive encoding of figure-ground relationships.  As well, in investigating the endogenous influences on figure-ground encodings, subjects were given a task of reading a book while recorded for EEG.  Onsets of changing note patterns elicited mismatch negativities in primary auditory cortex, indicating grouping can occur outside of the task at hand.

The research stemming from change deafness, auditory gist, and MMN have motivated that we are capable of producing unconscious representations in perception.  Further, these representations are even demonstrated across a wide range of modalities (visual MMN is discussed in Section \ref{sec:vmmm}) and further demonstrate the brains ability to segregate and in some cases integrate perceptual phenomena without the need for conscious awareness.  

\section{Vision}

In a seminal study on visual perception published in 1921, Rubin describes a fundamental form of experience consisting of a figure standing on a ground.  The figure describes the focal or fundamental experience of a scene, whereas the ground describes the ambient or marginal portions of a scene.  Expanding this point further in the 1920's, the Gestalt psychologists developed a comprehensive perceptual theory employing figure-ground as a fundamental type of perception where the notion of a Gestalt, or totality, is described by a figure and ground.  The Gestalt describes the fundamental experience of perception.  As a result, any subdivision or interrogation of a part of the Gestalt would alter experience into yet another figure and ground relationship \cite{Wever1927}.  

From these seminal studies, visual perception research has continued in trying to understand how the brain supports the formation and understanding of such figures.  The physiology and behavior of the eye has given researchers in visual attention and active visual cognition a unique window into the brain.  Starting with the physical wavelengths of light entering our eyes, we rapidly shift our gaze an average of 3-5 times a second, completely disrupting the continuity of light entering our eyes.  Visual acuity limitations mean that our eyes require rapid ballistic movements of the eye taking all of 30 ms (a \textit{saccade}) to project the light from the particular point of a visual scene we are interested in onto a 2-degree area of the retina with the highest spatial resolution (the \textit{fovea}).  Going away from the fovea (the \textit{parafovea}), resolution for spatial detail drops logarithmically, while resolution for motion detail increases, a relationship due to the distribution of photo-receptive cells in the eye combined with the lens of the eye itself.  

As our parafovea has been engineered for high motion resolution rather than high spatial resolution, we cannot encode with high spatial detail an entire visual scene at once, as a camera with a small aperture may be able to do.  Instead, we require an active viewing of a scene in order to perceive the details of a scene.  We do this through the use of saccades and head-movements to move and finally stabilize (a \textit{fixation}) our eyes to the region of interest, a process lasting on average 330 ms.  During this time, it is thought that the encoding of details at the point of fixation into memory occurs as well as planning of the next eye-movement \cite{}.  

\subsection{Attention}

The focus of research in eye-movement behavior has investigated why, where, and in which circumstances we move our eyes.  The earliest studies \cite{Buswell1935,Yarbus1967} describe two main influences of a viewer's attention to a visual scene: (1) influences dependent on mental states which focus attention towards contextually and cognitively relevant aspects of the world (\textit{endogenous}), and (2) influences dependent on involuntary capture of attention from the external environment (\textit{exogenous}).  As exogenous factors are involuntary, one would expect to find the behavior influenced by these factors to be highly consistent across viewers.  In contrast, as endogenous influences are dependent on cognitive factors resulting from emotion, memory, language, task, and previous experiences, the relation of a scene and one's endogenous influences on the scene are much less consistent across viewers.  

\subsubsection{Exogenous Influences on Attention}

In seminal work investigating the speed of visual perception using Gestalt primitives, Sziklai demonstrated the human visual system exhibits an attentional bottleneck of 40 bits per second on selected information, suggesting our visual systems require a simplified representation from the many megabytes per second of information coming from exogenous visual information \cite{Sziklai1956,Merrill1968}.  Much research investigating exogenous influences on static visual scenes therefore describe a simplified representation of attentional control known as a \textit{bottom-up} model \cite{Koch1985,Itti1998,Wolfe1989,Itti2001}.  Such models are built around theories of feature-integration \cite{Treisman1980} and are further supported by physiological evidence of the receptive fields and visual architecture of the visual cortex of cats \cite{Hubel1962}.  To discover the attentional biases for portions of a scene (\textit{saliency}), bottom-up models recompose a full resolution image using filter banks tuned to multiple frequency orientations and scales corresponding to pre-attentive visual features also found in early visual cortex such as luminance, oriented edges, and color contrasts.  Saliency is then computed as a weighted linear summation (\textit{integration}) of the resulting ``feature maps'' formed of different scales. 

It is thought that basic feature levels of models of integration are modulated by what Itti et. al calls ''top-down'' influences \cite{Itti2001} such as the current ongoing task \cite{Yarbus1967,Smith2011a} and the context of a scene in order to reduce processing load \cite{Henderson2003,Torralba2006}.  Though, the level at which top-down influences may affect processing is still open to debate.  Further, though these modulations are often described as top-down influences, such a term should not be confused with endogenous influences, as much research has shown that memory, context, and other endogenous factors affect early visual processing \cite{Tatler2011} which would correlate with initial feature stages thought to be unaffected in a bottom-up model.  Further, gist-based models (presented later) separating ``conceptual'' and ``perceptual'' influences may similarly be looked at in terms of ``bottom-up'' and ``top-down'' influences, or even ``exogenous'' and ``endogenous''.  As a result, these terms all vary slightly in their use and meaning, which should be no surprise given the complicated nature of the brain.  For the purposes of this thesis, endogenous and exogenous seem to offer the most useful definition separating influences originating from the brain and from the sensorial world, respectively, and other distinctions, e.g. bottom-up/top-down, are not used. 

Clustering of gaze during dynamic scene viewing -> motivate flicker as an attentional map, prior to understanding temporal incoherences, sets basis for proto-object representation later... discuss attentional synchrony paradigm... 
  	
\subsubsection{Endogenous Influences on Attention}
\label{sec:endogenous-influences}

In a seminal study on how task affects eye-movements during static scene viewing, \cite{Yarbus1967} tracked the eye-movements of participants viewing a painting entitled, ``An Unexpected Visitor.''  His study showed that when participants viewed the painting and were given a task such as to determine the ages of the people in the painting, they looked more at the faces of each person.  When asked to determine what they were wearing, their eye-movements strayed away from faces, and looked more towards the clothing of people.  Yarbus further describes 7 different tasks and shows how the eye-movements of each participant reflects the information required for processing the task at hand.  It is thought that task, therefore, is an endogenous influence.

In a similar study on dynamic scene viewing, Smith studied task-based effects on viewers' eye-movements looking at unedited videos of natural scenes from a camera mounted on a tripod \cite{Smith2011a}.  Participants were natives to the city of Edinburgh and viewed a variety of indoor and outdoor scenes from the city.  The study revealed that during free-viewing, i.e. not given any task other than to look at the video, participants looked at mostly moving objects such as people moving across the frame or cars.  However, when given the task to identify the location of the presented scene, participants had to concentrate their gaze towards the elements of a scene depicting landmarks such as buildings, signs, and trees and showed a remarkable ability to distract away from moving objects.  After viewers pressed a button indicating recognition of the location, their viewing behavior reverted to resembling the free-viewing task, fixating on moving objects such as people and cars again.  The study re-asserts the findings of Yarbus, though for a dynamic time-course.  Further, it also provides evidence of default viewing conditions during the time-course of viewing, as participants were able to ``return'' to the free-viewing task after having finished the task of recognizing the location of the scene.  

Discuss use of attentional synchrony paradigm in smith paper and in Melissa's paper too...

\subsection{Gist}
\label{sec:gist}

<<Introduce RSVP. then introduce gist more before saying what its findings entail... >>

The ability to classify scenes with rapid pre-attentive processing lasting only 45-135 ms (\textit{Gist}) \cite{Potter1969,Biederman1974,Potter1976,Schyns1994,Henderson1999} suggests that the general shape and structure of a scene leading one to infer its context are defined by either volumetric forms (\textit{geons}) \cite{Biederman1987}, spatial arrangement of blobs defined by contrasts in luminance or color \cite{Schyns1994,Oliva1997} or by using a scene's spatial frequency content \cite{Oliva2001,Oliva2005}.  A scene's spatial frequency content can be described by oriented band-pass filters: at a low spatial frequency, this content resembles broad edges and the layout and orientations of a scene's largest similarly textured regions, whereas at a high-spatial frequency, the response of the sharpest edges and their directions are encoded.  

Endogenous influences on subsequent processing of gist seem to influence the spectral scale at which gist is selected \cite{Schyns1994,Oliva1997}.  Schyns and Oliva describe an experiment where a low-spatial frequency (\textit{LSF}) and a high spatial frequency (\textit{HSF}) image are created for two separate pairs of images.  Creating two new images by combining the LSF of one image and the HSF of the other, and vice-versa, they investigate the scale space of gist recognition with and without a verbal cue to indicate what type of scene will follow (\textit{priming}).  Without priming, subjects are able to recognize the scene described by the LSF content of an image given 45 ms of presentation time, and the HSF one within 135 ms.  As well, subjects are unaware of the content in the other scale space (i.e. shown an image with LSF and HSF content for 45 ms, the participants are unaware of there being separate HSF content).  However, being primed with either the LSF or HSF content of the scene, subjects report perceiving the given cue instead.  

While gist is thought to be pre-attentive, i.e. before the timescale of acts of selective attention, such research suggests either that (1) the scale at which the early representation of gist operates at is affected by task-demands (i.e. only one scale of gist is encoded for pre-attentively), or (2), attention and further encoding into memory is dependent on endogenous influences on scale selection, (i.e. gist may be encoded at multiple scales, but only the scale selected by attentional machinery is encoded into memory).  Though not all scales are necessary for determining a scene's content when given prior cues (\textit{priming}), the neurobiology of early visual cortex gives scope for encoding of multiple visual scales.  It thus seems possible to assume (2) is a more likely model for the interaction of gist and attentional machinery.

<<Talk about features diagnostic to a scene's description, and how they accelerate or impair recognition of a scene, e.g. color in natural environments (Oliva, 2005).>>

Oliva further argues for two types of gist, \textit{conceptual} and \textit{perceptual}.  Conceptual gist refers to semantic information inferred from viewing a scene or from shortly after a scene disappears. Perceptual gist is thought to be motivated by the given task at hand in order to provide the structure or form of a scene and uses pre-attentive low-level information such as color, orientation, depth, and motion (Oliva, 2005).  <<Need to explain these much more as they go back into proto-objects and should be related to endo/exo as well etc...>>
	
Gist has been understood in terms of the classic rapid serial visual paradigm (RSVP), or within a screen based presentation where stimulus presentations are preceded by empty or noisy screens.  However, the real-world is not preceded by such screens, and rather the notion of gist across saccades, in a situated world becomes an important one to make.  How does a dynamic model of gist help us to maintain a coherent perspective of the world, and further guide our attention?  

\subsection{Change and Inattentional Blindness}

Research demonstrating the failure to report large changes in the visual world (\textit{change blindness}) as well as the failure to report unexpected visible changes due to task requiring attention elsewhere (\textit{inattentional blindness}) \cite{Simons1999,Rensink2000,Rensink2001,Hollingworth2001a} have shown that our visual systems are unaware of changes in visual world outside of the point of fixation.  Simons and Chabris demonstrated '\textit{Inattentional Blindness} by composing a video of two basketball teams dressed in white and black passing a ball to each other \cite{Simons1999}.  Participants were asked to count the number of passes that the white team makes.  During the course of the video, a person wearing a gorilla suit walks across the frame of the camera, unnoticed by 75\% of participants.  The phenomena of \textit{Change Blindness} was demonstrated in a real-world psychology experiment \cite{Simons1998} where participants arrived at a kiosk to fill in a consent form and hand the completed form to a man behind the counter.  The man ducks behind the counter as to pretend to file the paper, while a different man comes up from behind the counter, again unnoticed by a majority of the participants.  

Failing to detect changes outside of the point of fixation suggests that any peripheral representation of a scene would likely not encode details of object specific features such as color, motion, or orientation gratings.  Rather, our visual machinery integrates the detailed aspects of objects across eye-movements, retaining that information as a perceived representation of the visual world.  The broad spatial scale afforded by the lens of the eye and the higher motion resolution afforded by the spacing of cones in the periphery give further indication to the lack of highly detailed feature encoding in the periphery.  Though, to what form, and to what detail a periphery representation may encode is still an open question.  

Rensink takes this evidence in developing a theory of coherence, proposing that object representation depends on focal attention.  For objects outside of the point of fixation, Rensink proposes we encode volatile units of \textit{proto-objects} \cite{Rensink2000,Rensink2001}.  Proto-objects are argued to be amorphous and blob-like in nature, representational-less and concept-less lasting only a few hundred milliseconds.  It is further argued that attention operates on groupings of proto-objects rather than at the earlier feature levels making it the highest level of early vision, and the earliest operands of selective attention.  Rensink also hypothesizes that proto-objects may explain non-attentive processes capable of recognizing the abstract meaning of a scene and the spatial layout of the scene \cite{Rensink2002}.  In relation to perceptual influences, implicit behavioral measures suggest that grouping processes can also occur for task-irrelevant visual stimuli, i.e., for stimuli that has not been attended to by a fixation, further supporting theories of proto-object formation \cite{Lamy2006}.

\subsection{Electrophysiology}\label{sec:electrophysiology}

Visual Mismatch Negativity... reviews in Czigler 2007, 2010...
\cite{Stefanics2011}

Violation of sequential rules elicited for a deviant color (Czigler 2002), orientation (Kimura 2010), movement (Pazo-Alvarez 2004), spatial frequency (Heslenfeld 2003), contrast (Stagg 2004), sequential conditional relationships (Stefanics, 2011), and even facial expressions (Zhao and Li, 2006)

%Marr, invariances...


\section{Discussion}

Research in change blindness has indicated that though we experience a rich, detailed visual world, we do not use such rich details in building a stable representation \cite{Simons1997}.  Rensink argues that object representation requires focal attention.  However, in considering an architecture of visual perception, what is the cause of producing focal attention?  The literature presented here suggests that there is either an endogenous explanation or exogenous one.  For example, I may focus on a cup, but not build the representation of the fingerprints on the cups as I was not intending to look at this particular scale.  In this case, the endogenous influence of perceiving the object representation of fingerprints on the cup was necessary for building such a representation, even though focal attention will have brought my eyes to the cup.  It may be that my task of drinking from the cup saw the cup as what it afforded: a drink.  In a free-viewing task, if such a thing exists, it may be more likely that an exogenous influence such as the mis-representation of the cup will provoke more detailed representations and cause additional focal attention to the cup.  Thus, it may be the case that focal attention is necessary for explaining an object, however, it seems it is not sufficient and the cause of focal attention should still be considered.  

When considering evidence for gist in relation to Rensink's theory of coherence, it seems viable to consider proto-objects as the same representation that gist may use \cite{Rensink2002}.  Though Schyns and Oliva argue for using oriented banded filters, it is not unlikely that collections of blob-like entities which necessarily also respond to the scale of the proto-object could provide a cue for spatial layout.   However, when considering evidence in rapid determination of the meaning of scenes, Schyns and Oliva demonstrated that early processing of a scene could be re-organized based on prior experiences \cite{Schyns1994,Oliva1997}.  Thus, it is not clear from their research alone whether the pre-conceptual representation itself can be changed, or if only the attentional machinery acting on a set of possible representations has changed.  The latter effect would entail a sort of conceptual prior on a scene, suggesting the organization of a scenes early representation remains untouched.

Pylyshyn theorizes that the understanding of a concept is not all that is required for visual experience: 
\begin{quotation}
''Vision suited for the control of action will have to provide something more than a system that constructs a conceptual representation from visual stimuli; it will also need to provide a special kind of direct (preconceptual, unmediated) connection between elements of a visual representation and certain elements in the world. Like natural language demonstratives (such as 'this' or 'that') this direct connection allows entities to be referred to without being categorized or conceptualized. \cite{Pylyshyn2001}''
\end{quotation}  
The preconceptual connections Pylyshyn describes are easily described by the pre-attentive proto-objects Rensink also describes \cite{Rensink2000,Rensink2001}.  What is interesting in Pylyshyn's theory is the notion that this pre-conceptual representation does not need to be categorized or conceptualized in order to be referred to.  In other words, the categorization which Pylyshyn theorizes of is part of the attentional machinery which refers to proto-objects, rather than an explicit property of the proto-object themselves.  According to Pylyshyn's theory, proto-objects of a visual scene are then described by one particular fate, and attentional mechanisms can only select from the set of possible proto-objects, rather than influence their definition. 

Proto-objects act as an indexical reference to a conceptual referent.  How proto-objects are defined is based on the features that best give coherence to the scene.  They are also scaled with increasing eccentricity, presumably at least because of the shape of the eye would decrease spatial resolution, but as well due to the nature of attention acting at the point of fixation.  Functionally, visual material outside of the region is of less importance to the task at hand.  Thus, the biological nature of the shape of the eye could also be understood in terms of evolving to act in the world.  Proto-objects may also possibly find re-definition from the act of attention.  As proto-objects are volatile in nature, the act of attention reshapes ones perspective of a scene, thus reorganizing the boundaries defining proto-objects, even at the point of attention.

Global shape as in the gist of a scene gives a context for defining the possible scale of proto-objects.  As well, the collection of proto-objects themselves can be understood as a scene's shape.  In audition, the shape could be defined as the timbre, or what frequencies the sprectrum may generalize to, as in MFCC or LFCC features.  Proto-objects for audition may be thought of in terms of independent components in a matrix factorization of a time-frequency matrix.  These components map to gestalts of spectral information.  They are also understood in the Bregman sense of streams as they are perceptual units that are also volatile depending on the current auditory information and perception of the viewer.

\section{Conclusion}

Considering both the implicit, unmediated representation and the attentional and contextual mechanisms, at least two critical layers should be built into any computational model based on the evidence presented here:  (1), a pre-conceptual representation which takes into account different possible spatial configurations, composed of either band-passed edge-oriented filters, geons, or proto-objects, where this representation is affected by a logarithmic filter around the point of fixation based on the evidence of response properties of photo-receptors; (2), an attentional and contextual influence supported by the ongoing experiences of the subject such that parafoveal information becomes unstable without ongoing attention and is only inferred by through the context of the scene.  The intentions of an agent within this model are still not well-understood, as the variety of possible endogenous influences that may be possible are too great.  

Similar computational models have been developed to explain visual perception machinery \cite{Walther2006,Orabona2007a}, however they each suffer from a number of problems: (1) they lack the inclusion of the evidence of the response properties of photoreceptors in the retina as there is no indication of the current or ongoing attention within the visual scene; (2) they infer context based on solely a static image whereas the real-world is dynamic; and (3), they cannot distinguish groupings of proto-objects and instead create discrete maps which are thresholded as attention or saliency maps.  Furthermore, the interest in the previously cited models of visual perception is in predicting attention towards a scene, rather than allowing an agent in the world to explicitly define this.  In such a case, these models are unsuitable for applications in augmented or virtual reality where the agent already provides attention within a scene.  

